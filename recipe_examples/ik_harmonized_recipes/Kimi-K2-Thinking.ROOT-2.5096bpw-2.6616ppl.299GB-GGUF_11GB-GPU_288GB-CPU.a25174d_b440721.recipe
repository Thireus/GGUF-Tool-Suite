## Quant mix recipe created using Thireus' GGUF Tool Suite - https://gguf.thireus.com/
# Model name: Kimi-K2-Thinking
# Link to the original model: https://huggingface.co/moonshotai/Kimi-K2-Thinking

## Model head & embeddings — qbits: 32 8 6 
^output_norm\.weight$=f32
^output\.weight$=q8_0
^token_embd\.weight$=iq6_k

## Special attention kernels — single-quant only (llama-quantize takes care of it) — qbits: 8 
^blk\.([0-9]|[1-5][0-9]|60)\.attn_k_b\.weight$=q8_0

## Multi-headed attention parameters — qbits: 32 8 6 
^blk\.([0-9]|[1-5][0-9]|60)\.attn_norm\.weight$=f32
^blk\.([0-9]|[1-5][0-9]|60)\.attn_kv_a_mqa\.weight$=q8_0
^blk\.([0-9]|[1-3][0-9]|4[0-3]|45|4[7-9]|50|5[3-9]|60)\.attn_output\.weight$=q8_0
^blk\.([0-9]|[1-5][0-9]|60)\.attn_q_a\.weight$=q8_0
^blk\.([0-9]|[1-5][0-9]|60)\.attn_q_a_norm\.weight$=f32
^blk\.([0-9]|[1-5][0-9]|60)\.attn_q_b\.weight$=q8_0
^blk\.([0-9]|[1-5][0-9]|60)\.attn_v_b\.weight$=q8_0
^blk\.(44|46|5[1-2])\.attn_output\.weight$=iq6_k
^blk\.([0-9]|[1-5][0-9]|60)\.attn_kv_a_norm\.weight$=f32

## Dense Feed-Forward Network weights — qbits: 8 6 
^blk\.0\.ffn_down\.weight$=q8_0
^blk\.0\.ffn_up\.weight$=iq6_k

## MoE Gating & Routing — qbits: 32 
^blk\.([1-9]|[1-5][0-9]|60)\.ffn_gate_inp\.weight$=f32
^blk\.([1-9]|[1-5][0-9]|60)\.exp_probs_b\.bias$=f32

## Gating network — qbits: 5 
^blk\.0\.ffn_gate\.weight$=iq5_k_r4

## Misc / Other tensors — qbits: 32 
^blk\.([0-9]|[1-5][0-9]|60)\.ffn_norm\.weight$=f32

## GPU-loaded - MoE Shared Experts Feed-Forward Network - ffn_*_shexp
# ffn_down_shexp — down-projection (shared experts) — qbits: 8 6 
^blk\.([1-9]|1[0-9]|2[0-8]|3[0-3]|5[6-9]|60)\.ffn_down_shexp\.weight$=q8_0
^blk\.(29|3[4-9]|4[0-9]|5[0-5])\.ffn_down_shexp\.weight$=iq6_k

# ffn_up_shexp — up-projection (shared experts) — qbits: 8 6 
^blk\.([1-9]|1[0-9]|2[0-8]|3[0-3]|5[6-9]|60)\.ffn_up_shexp\.weight$=q8_0
^blk\.(29|3[4-9]|4[0-9]|5[0-5])\.ffn_up_shexp\.weight$=iq6_k

# ffn_gate_shexp — gating network (shared experts) — qbits: 8 6 
^blk\.([1-9]|1[0-9]|2[0-8]|3[0-3]|5[6-9]|60)\.ffn_gate_shexp\.weight$=q8_0
^blk\.(29|3[4-9]|4[0-9]|5[0-5])\.ffn_gate_shexp\.weight$=iq6_k

## CPU-friendly - MoE Per-expert Feed-Forward Network - ffn_*_exps
# ffn_down_exps — down-projection (per-expert) — qbits: 3 2 
^blk\.([2-9]|1[0-4]|1[7-8]|3[4-9]|4[0-3]|46|48)\.ffn_down_exps\.weight$=iq3_kt
^blk\.(1|1[5-6]|19|2[0-9]|3[0-3]|4[4-5]|47|49|5[0-9]|60)\.ffn_down_exps\.weight$=iq2_kt

# ffn_up_exps — up-projection (per-expert) — qbits: 3 2 
^blk\.([2-9]|1[0-2]|4[1-2]|46|48)\.ffn_up_exps\.weight$=iq3_kt
^blk\.(1|1[3-9]|[2-3][0-9]|40|4[3-5]|47|49|5[0-9]|60)\.ffn_up_exps\.weight$=iq2_kt

# ffn_gate_exps — gating network (per-expert) — qbits: 3 2 
^blk\.([2-9]|1[0-2]|4[1-2]|46|48)\.ffn_gate_exps\.weight$=iq3_kt
^blk\.(1|1[3-9]|[2-3][0-9]|40|4[3-5]|47|49|5[0-9]|60)\.ffn_gate_exps\.weight$=iq2_kt

## Summary of tensor sizes per class
# GPU Total: 11.45 GiB (95.0%) | 12.05 GiB max, if all were q8_0 | 8.11 GiB min, if all were iq5_k_r4
# CPU Total: 288.42 GiB (78.1%) | 369.14 GiB max, if all were iq3_kt | 206.72 GiB min, if all were iq1_kt
# GPU+CPU Total: 299.87 GiB (86.6%)

## Summary of tensor counts and bpw per qtype
#
# GPU-loaded quants:
# QTYPE		Count	BPW	Assigned GiB	% Assigned	Max GiB (all)
# +f32       	365	32    	  0.62 GiB	-		-
# +q8_0      	61 	8.5   	  0.25 GiB	-		-
# q8_0      	414	8.5   	  8.52 GiB	76.2%		11.18
# iq6_k     	75 	6.625 	  1.97 GiB	22.6%		8.72
# iq5_k_r4  	1  	5.5   	  0.08 GiB	1.2%		7.24
#
# CPU-friendly quants:
# QTYPE		Count	BPW	Assigned GiB	% Assigned	Max GiB (all)
# iq3_kt    	57 	3.125 	116.89 GiB	31.7%		369.14
# iq2_kt    	123	2.125 	171.53 GiB	68.3%		251.02
# iq1_kt    	0  	1.75  	  0.00 GiB	0.0%		206.72
#
# -Average BPW: 2.5096
#
# -Notes:
# - '+' means user-defined pre-assigned tensors, or tensor missing from csv data or f32 tensors
# - Recipe produced on the 2025-12-28 10:10:43 UTC+0000 using Thireus' GGUF tools (https://gguf.thireus.com/)
# - Script SHA-256: a25174dd245fa8b2cc086752c5c0a8e360786d6828e287f627fea03fceee984c
# - Calibration dataset 'kld_results.csv' SHA-256: b5a7341cb88828e49793a106f43de1312681162500304922e26ab9cc30292203
# - Degradation dataset 'group0/kld_results.csv' SHA-256: 1686c5139bc99414a43bad47fdafceb360ee9b9cb701ba425cc9dc386547cded
# - tensors.bf16.map SHA-256: b92fda9c9fcee52780a84dbc1772dbb9b967a3ca5901d981258257bdd5bf30ca
# - tensors.bf16.map model name: Kimi-K2-Thinking-THIREUS-BF16-SPECIAL_TENSOR-01097-of-01097
# - tensors.q8_0.map SHA-256: 4b8106b4c534dfa9474a9881ce1959328e1bbb142307aa0e89bbcf8a6d58d10e
# - tensors.q8_0.map model name: Kimi-K2-Thinking-THIREUS-Q8_0-SPECIAL_TENSOR-01097-of-01097
# - tensors.iq6_k.map SHA-256: bbc3a96641b51468cccb20b9647abe6a7daa7c4cbf139d8a353b713d8446a9e6
# - tensors.iq6_k.map model name: Kimi-K2-Thinking-THIREUS-IQ6_K-SPECIAL_TENSOR-01097-of-01097
# - tensors.iq5_k_r4.map SHA-256: be395c01a3dbc2fc43d7020bdb18c4148e25971325874046d36b451618378ffb
# - tensors.iq5_k_r4.map model name: Kimi-K2-Thinking-THIREUS-IQ5_K_R4-SPECIAL_TENSOR-01097-of-01097
# - tensors.iq3_kt.map SHA-256: 6f84dd7a52dc18f4aa521e4c1e0d2e0c106f986c18c30862111148c182fcc0e1
# - tensors.iq3_kt.map model name: Kimi-K2-Thinking-THIREUS-IQ3_KT-SPECIAL_TENSOR-01097-of-01097
# - tensors.iq2_kt.map SHA-256: baa743da76e757d237f1145d258c7505d6345c1ccd098d939ecf24c178cf8070
# - tensors.iq2_kt.map model name: Kimi-K2-Thinking-THIREUS-IQ2_KT-SPECIAL_TENSOR-01097-of-01097
# - tensors.iq1_kt.map SHA-256: be45178bc1c81f002e2f8a490cfad46ef4d771d2240a2a7a0de2e4279be56faa
# - tensors.iq1_kt.map model name: Kimi-K2-Thinking-THIREUS-IQ1_KT-SPECIAL_TENSOR-01097-of-01097
# - GPG signatures: PASSED
# - Command used:
# ../../quant_assign.py kld_results.csv --use-greedy-quant-assign --quant-degradation-csv group0/kld_results.csv \
# --quant-degradation-equation 'y = -0.0772258662462 + 0.399144361667 * ( x - 1.31650903625 )^(-1.41531100478)' \
# --tolerance 0.01 --cpu-irq-k 1.5 --gpu-irq-k 1.5 --gpu-assign-qtype iq6_k --cpu-tensors-max-size 288.5 \
# --gpu-tensors-max-size 95% --exponential-factor 8 --cpu-tensors 'blk\..*\.ffn_down_exps\.weight' \
# 'blk\..*\.ffn_up_exps\.weight' 'blk\..*\.ffn_gate_exps\.weight' --gpu-tensors '.*' --cpu-quants iq3_kt iq2_kt \
# iq1_kt --gpu-quants q8_0 iq5_k_r4 iq6_k --gpu-assign-tensors '^blk\.([0-9]|[1-5][0-9]|60)\.attn_k_b\.weight$=q8_0' \
# --harmonize-tensors '^blk\..*\.ffn_up_exps.*,blk\..*\.ffn_gate_exps.*' --harmonization-technique 3

## THE END!
