## Quant mix recipe created using Thireus' GGUF Tool Suite - https://gguf.thireus.com/
# Model name: GLM-4.5-Air
# Link to the original model: https://huggingface.co/zai-org/GLM-4.5-Air

## Model head & embeddings — qbits: 32 8 5 
^output_norm\.weight$=f32
^token_embd\.weight$=q5_K
^output\.weight$=q8_0

## Multi-headed attention parameters — qbits: 32 5 
^blk\.([0-9]|[1-3][0-9]|4[0-6])\.attn_norm\.weight$=f32
^blk\.([0-9]|[1-3][0-9]|4[0-6])\.attn_v\.weight$=q5_K
^blk\.([0-9]|[1-3][0-9]|4[0-6])\.attn_output\.weight$=q5_K
^blk\.([0-9]|[1-3][0-9]|4[0-6])\.attn_v\.bias$=f32
^blk\.([0-9]|[1-3][0-9]|4[0-6])\.attn_q\.bias$=f32
^blk\.([0-9]|[1-3][0-9]|4[0-6])\.attn_q\.weight$=q5_K
^blk\.([0-9]|[1-3][0-9]|4[0-6])\.attn_k\.bias$=f32
^blk\.([0-9]|[1-3][0-9]|4[0-6])\.attn_k\.weight$=q5_K

## Dense Feed-Forward Network weights — qbits: 8 
^blk\.0\.ffn_down\.weight$=q8_0
^blk\.0\.ffn_up\.weight$=q8_0

## NextN tensors — qbits: 32 5 
^blk\.46\.nextn\.enorm\.weight$=f32
^blk\.46\.nextn\.embed_tokens\.weight$=q5_K
^blk\.46\.nextn\.shared_head_head\.weight$=q5_K
^blk\.46\.nextn\.shared_head_norm\.weight$=f32
^blk\.46\.nextn\.eh_proj\.weight$=q5_K
^blk\.46\.nextn\.hnorm\.weight$=f32

## MoE Gating & Routing — qbits: 32 
^blk\.([1-9]|[1-3][0-9]|4[0-6])\.ffn_gate_inp\.weight$=f32
^blk\.([1-9]|[1-3][0-9]|4[0-6])\.exp_probs_b\.bias$=f32

## Gating network — qbits: 6 
^blk\.0\.ffn_gate\.weight$=q6_K

## Misc / Other tensors — qbits: 32 
^blk\.([0-9]|[1-3][0-9]|4[0-6])\.post_attention_norm\.weight$=f32

## GPU-loaded - MoE Shared Experts Feed-Forward Network - ffn_*_shexp
# ffn_down_shexp — down-projection (shared experts) — qbits: 6 
^blk\.([1-9]|[1-3][0-9]|4[0-6])\.ffn_down_shexp\.weight$=q6_0

# ffn_up_shexp — up-projection (shared experts) — qbits: 8 6 5 
^blk\.(1|[3-4]|[7-8]|13|17|19|23|28|30|34|4[2-5])\.ffn_up_shexp\.weight$=q8_0
^blk\.([5-6]|[1-2][0-2]|2[4-5]|27|29|3[2-3]|3[5-9]|4[0-1]|46)\.ffn_up_shexp\.weight$=q6_K
^blk\.(2|9|1[4-6]|18|26|31)\.ffn_up_shexp\.weight$=q5_K

# ffn_gate_shexp — gating network (shared experts) — qbits: 8 6 5 
^blk\.(1|3|[7-9]|11|20|23|4[2-5])\.ffn_gate_shexp\.weight$=q8_0
^blk\.(2|6|10|12|15|19|2[1-2]|2[4-9]|3[0-9]|4[0-1]|46)\.ffn_gate_shexp\.weight$=q6_K
^blk\.([4-5]|1[3-4]|1[6-8])\.ffn_gate_shexp\.weight$=q5_K

## CPU-friendly - MoE Per-expert Feed-Forward Network - ffn_*_exps
# ffn_down_exps — down-projection (per-expert) — qbits: 4 
^blk\.([1-9]|[1-3][0-9]|4[0-6])\.ffn_down_exps\.weight$=iq4_nl

# ffn_up_exps — up-projection (per-expert) — qbits: 6 5 4 
^blk\.(25|29|31|37|46)\.ffn_up_exps\.weight$=q6_K
^blk\.(3|10|12|15|2[0-3]|2[6-8]|30|3[2-6]|3[8-9]|4[0-5])\.ffn_up_exps\.weight$=q5_K
^blk\.([1-2]|[4-9]|11|1[3-4]|1[6-9]|24)\.ffn_up_exps\.weight$=iq4_xs

# ffn_gate_exps — gating network (per-expert) — qbits: 6 5 4 
^blk\.(10|29|3[1-2]|34|46)\.ffn_gate_exps\.weight$=q6_K
^blk\.(3|6|1[4-6]|19|2[3-8]|30|33|3[6-9]|4[0-2]|4[4-5])\.ffn_gate_exps\.weight$=q5_K
^blk\.([1-2]|[4-5]|[7-9]|1[1-3]|1[7-8]|2[0-2]|35|43)\.ffn_gate_exps\.weight$=iq4_xs

## Summary of tensor sizes per class
# GPU Total: 5.96 GiB (94.9%) | 6.28 GiB max, if all were q8_0 | 5.63 GiB min, if all were q5_K
# CPU Total: 58.73 GiB (84.3%) | 69.67 GiB max, if all were q6_K | 51.79 GiB min, if all were iq4_xs
# GPU+CPU Total: 64.69 GiB (89.6%)

## Summary of tensor counts and bpw per qtype
#
# GPU-loaded quants:
# QTYPE		Count	BPW	Assigned GiB	% Assigned	Max GiB (all)
# +f32       	331	32    	  0.09 GiB	-		-
# +q8_0      	1  	8.5   	  0.04 GiB	-		-
# q8_0      	30 	8.5   	  0.82 GiB	44.4%		1.84
# q6_K      	50 	6.5625	  0.25 GiB	17.6%		1.42
# *+q6_0    	46 	6.5   	  0.20 GiB	-		-
# +q5_K      	191	5.5   	  4.10 GiB	-		-
# q5_K      	16 	5.5   	  0.45 GiB	38.0%		1.19
#
# CPU-friendly quants:
# QTYPE		Count	BPW	Assigned GiB	% Assigned	Max GiB (all)
# +q6_K      	2  	6.5625	  1.13 GiB	-		-
# q6_K      	9  	6.5625	  5.08 GiB	10.0%		50.76
# q5_K      	48 	5.5   	 22.69 GiB	53.3%		42.54
# +iq4_nl    	46 	4.5   	 17.79 GiB	-		-
# iq4_xs    	33 	4.25  	 12.05 GiB	36.7%		32.87
#
# -Average BPW: 5.0303
#
# -Notes:
# - '+' means user-defined pre-assigned tensors, or tensor missing from csv data or f32 tensors
# - '*' means fallback tensors: these tensors were present in the map(s) with a different dtype than the originally-intended qtype;
#   they have been grouped and displayed as '*<qtype>' above to show the final (map-observed) qtype and sizes separately.
# - WARNING: 46 tensor assignments were substituted to the dtype actually present in their tensor map files. 
#   This may change the final size relative to the expected thresholds and chosen quants. 
#   To disable automatic map-based fallbacks and preserve the script's original assigned qtypes exactly, re-run with --no-fallback.
# - Recipe produced on the 2025-11-21 09:45:10 GMT+0000 using Thireus' GGUF tools (https://gguf.thireus.com/)
# - Script SHA-256: 569b7f6a3239c9173d71ca1fadf34222607d72a2cfed2c284b42633e95b4a627
# - Calibration dataset 'ppl_results.csv' SHA-256: c596235f01c582988d23f97e1e6809a83923ae3f5321e3cde00625c9c92952f3
# - tensors.bf16.map SHA-256: f440313db9b7ce593240c0b0acb723182ee3ae9570eca868dc6eb440112fdd67
# - tensors.bf16.map model name: GLM-4.5-Air-THIREUS-BF16-SPECIAL_TENSOR-00804-of-00804
# - tensors.q8_0.map SHA-256: c00093e70a6c32aab72b404457c12a7b238b0e030975267d93d2b09a30796151
# - tensors.q8_0.map model name: GLM-4.5-Air-THIREUS-Q8_0-SPECIAL_TENSOR-00804-of-00804
# - tensors.q6_K.map SHA-256: 5165939ae192b9008b49432f574da6df0a8df9989faf337cc3a062d04f80aef2
# - tensors.q6_K.map model name: GLM-4.5-Air-THIREUS-Q6_K-SPECIAL_TENSOR-00804-of-00804
# - tensors.q6_0.map SHA-256: 498117016ff0da4d6f5d6862776fb54ebe8a48eabf659005103531d25722d305
# - tensors.q6_0.map model name: GLM-4.5-Air-THIREUS-Q6_0-SPECIAL_TENSOR-00804-of-00804
# - tensors.q5_K.map SHA-256: b60aadca788055846a572cad5121e1d93bfa9bbbd520ae6350c84f52319f945f
# - tensors.q5_K.map model name: GLM-4.5-Air-THIREUS-Q5_K-SPECIAL_TENSOR-00804-of-00804
# - tensors.iq4_nl.map SHA-256: ed9419ab49fc319d033148d4241f28fcafb1d3089cb3172da9fa5f62c978a93d
# - tensors.iq4_nl.map model name: GLM-4.5-Air-THIREUS-IQ4_NL-SPECIAL_TENSOR-00804-of-00804
# - tensors.iq4_xs.map SHA-256: 28c799175c45409d6f59d609e82c5f0ed2bba3240b7c5697afbdc76824b1b046
# - tensors.iq4_xs.map model name: GLM-4.5-Air-THIREUS-IQ4_XS-SPECIAL_TENSOR-00804-of-00804
# - tensors.iq2_ks.map SHA-256: efed8f3d7d712a6ad99c5904f6e2f4b89387cc78e4008d9ca557bd04da1f2b31
# - tensors.iq2_ks.map model name: GLM-4.5-Air-THIREUS-IQ2_KS-SPECIAL_TENSOR-00804-of-00804
# - GPG signatures: PASSED
# - Command used:
# ../../quant_assign.py ppl_results.csv --tolerance 0.01 --cpu-irq-k 1.5 --gpu-irq-k 1.5 --gpu-assign-qtype q5_K \
# --cpu-tensors-max-size 59 --gpu-tensors-max-size 95% --exponential-factor 8 --cpu-tensors \
# 'blk\.([1-9]|[1-3][0-9]|4[0-5])\.ffn_up_exps\.weight' 'blk\.([1-9]|[1-3][0-9]|4[0-5])\.ffn_gate_exps\.weight' \
# --gpu-tensors '.*' --cpu-quants q5_K iq4_xs q6_K --gpu-quants q8_0 q5_K q6_K --cpu-assign-tensors \
# 'blk\.(46)\.ffn_up_exps\.weight=q6_K' 'blk\.(46)\.ffn_gate_exps\.weight=q6_K' \
# 'blk\.([1-9]|[1-3][0-9]|4[0-6])\.ffn_down_exps\.weight=iq4_nl' --gpu-assign-tensors \
# 'blk\.(0)\.ffn_down\.weight=q8_0' --harmonize-tensors '' --harmonization-technique 0

## THE END!
