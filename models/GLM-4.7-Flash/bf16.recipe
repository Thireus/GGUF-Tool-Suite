## Quant mix recipe created using Thireus' GGUF Tool Suite - https://gguf.thireus.com/

## Model head & embeddings — qbits: 32 16 
^token_embd\.weight$=bf16
^output\.weight$=bf16
^output_norm\.weight$=f32

## Special attention kernels — single-quant only (llama-quantize takes care of it) — qbits: 16 
^blk\.([0-9]|[1-3][0-9]|4[0-6])\.attn_k_b\.weight$=bf16

## Multi-headed attention parameters — qbits: 32 16 
^blk\.([0-9]|[1-3][0-9]|4[0-6])\.attn_norm\.weight$=f32
^blk\.([0-9]|[1-3][0-9]|4[0-6])\.attn_output\.weight$=bf16
^blk\.([0-9]|[1-3][0-9]|4[0-6])\.attn_kv_a_mqa\.weight$=bf16
^blk\.([0-9]|[1-3][0-9]|4[0-6])\.attn_q_a\.weight$=bf16
^blk\.([0-9]|[1-3][0-9]|4[0-6])\.attn_q_a_norm\.weight$=f32
^blk\.([0-9]|[1-3][0-9]|4[0-6])\.attn_v_b\.weight$=bf16
^blk\.([0-9]|[1-3][0-9]|4[0-6])\.attn_kv_a_norm\.weight$=f32
^blk\.([0-9]|[1-3][0-9]|4[0-6])\.attn_q_b\.weight$=bf16

## Dense Feed-Forward Network weights — qbits: 16 
^blk\.0\.ffn_down\.weight$=bf16
^blk\.0\.ffn_up\.weight$=bf16

## MoE Gating & Routing — qbits: 32 
^blk\.([1-9]|[1-3][0-9]|4[0-6])\.ffn_gate_inp\.weight$=f32
^blk\.([1-9]|[1-3][0-9]|4[0-6])\.exp_probs_b\.bias$=f32

## Gating network — qbits: 16 
^blk\.0\.ffn_gate\.weight$=bf16

## Misc / Other tensors — qbits: 32 
^blk\.([0-9]|[1-3][0-9]|4[0-6])\.ffn_norm\.weight$=f32

## GPU-loaded - MoE Shared Experts Feed-Forward Network - ffn_*_shexp
# ffn_down_shexp — down-projection (shared experts) — qbits: 16 
^blk\.([1-9]|[1-3][0-9]|4[0-6])\.ffn_down_shexp\.weight$=bf16

# ffn_up_shexp — up-projection (shared experts) — qbits: 16 
^blk\.([1-9]|[1-3][0-9]|4[0-6])\.ffn_up_shexp\.weight$=bf16

# ffn_gate_shexp — gating network (shared experts) — qbits: 16 
^blk\.([1-9]|[1-3][0-9]|4[0-6])\.ffn_gate_shexp\.weight$=bf16

## CPU-friendly - MoE Per-expert Feed-Forward Network - ffn_*_exps
# ffn_down_exps — down-projection (per-expert) — qbits: 16 
^blk\.([1-9]|[1-3][0-9]|4[0-6])\.ffn_down_exps\.weight$=bf16

# ffn_up_exps — up-projection (per-expert) — qbits: 16 
^blk\.([1-9]|[1-3][0-9]|4[0-6])\.ffn_up_exps\.weight$=bf16

# ffn_gate_exps — gating network (per-expert) — qbits: 16 
^blk\.([1-9]|[1-3][0-9]|4[0-6])\.ffn_gate_exps\.weight$=bf16



## THE END!